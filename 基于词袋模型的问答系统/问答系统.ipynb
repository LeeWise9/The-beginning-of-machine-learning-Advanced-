{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 项目：搭建一个简单的问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次项目的目标是搭建一个简答的问答系统，会用到以下几个知识点： 1. 字符串的操作  2. 不同数据结构的使用  3. 词袋模型   4. 相似度计算\n",
    "问答系统所需要的数据已经提供，对于每一个问题都可以找得到相应的答案，所以可以理解为每一个数据是 <问题、答案>。 那系统的核心是当用户输入一个问题的时候，首先要找到跟这个问题最相近的已经存储在库里的问题，然后直接返回相应的答案即可。 举一个简单的例子：\n",
    "\n",
    "假设我们的库里面已有存在以下几个<问题,答案>：\n",
    "<\"贪心学院主要做什么方面的业务？”， “他们主要做人工智能方面的教育”>\n",
    "<“国内有哪些做人工智能教育的公司？”， “贪心学院”>\n",
    "<\"人工智能和机器学习的关系什么？\", \"其实机器学习是人工智能的一个范畴，很多人工智能的应用要基于机器学习的技术\">\n",
    "<\"人工智能最核心的语言是什么？\"， ”Python“>\n",
    ".....\n",
    "\n",
    "假设一个用户往系统中输入了问题 “贪心学院是做什么的？”， 那这时候系统先去匹配最相近的“已经存在库里的”问题。 那在这里很显然是 “贪心学院是做什么的”和“贪心学院主要做什么方面的业务？”是最相近的。 所以当我们定位到这个问题之后，直接返回它的答案 “他们主要做人工智能方面的教育”就可以了。 所以这里的核心问题可以归结为计算两个问句（query）之间的相似度。\n",
    "\n",
    "在本次项目中，你会频繁地使用到sklearn这个机器学习库。具体安装请见：http://scikit-learn.org/stable/install.html  sklearn包含了各类机器学习算法和数据处理工具，包括本项目需要使用的词袋模型，均可以在sklearn工具包中找得到。 另外，本项目还需要用到分词工具jieba, 具体使用方法请见 https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一部分： 在这部分里，你首先需要去读取给定的文件，并把文件里的内容读取到list里面。这部分的任务主要需要文件IO操作方面的基本知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file):\n",
    "    # 读取文件里的内容，把读取后的结果写到数组里，并进行返回。 文件里的每一行都是一个记录，比如“Question_combined.data”文件里的每一行\n",
    "    # 都是一个独立的问题； 类似的 “Answer_combined.data”里的每一行都是对应的答案（如果百度知道里有多个回答，会以特殊符号|`|来隔开，\n",
    "    # 打开文件即可以看到）。 “Question_combined.data”里的每一行的问题，都对应“Answer_combined.data”里相应行的回答。 \n",
    "    \n",
    "    # TODO： 逐行读取文件，并把结果写入到一个list里面。 最后需要返回的list可能是这样的格式：\n",
    "    #   result = [\"这现象是不是显存坏了还是显示器老化?\",\n",
    "    #             \"急求外贸解答：我有个业务上的烦恼…\", \n",
    "    #             \"我买了惠普dv3 4048笔记本电脑，可是玩cf的画面很卡，谁能帮我调一下显卡\t我的显卡类型是ati5450\",       \n",
    "    #             .....\n",
    "    #            ]\n",
    "    #       也就是list of strings, 每一个string代表一个问题或者相应的回答（可能包含多个回答）。 在这不需要做任何的字符串处理，\n",
    "    #       只需要读入并把它写入到list即可。\n",
    "    # file: 输入文件\n",
    "    data = [] #空列表：存放data\n",
    "    with open(file,'r', encoding='utf-8') as file:\n",
    "        # TODO 提取每一个评论，然后利用process_line函数来做处理，并添加到comments。\n",
    "        for line in file:\n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 从文件读取问题和相应的回答   \n",
    "questions = read_corpus(\"Question_combined.dat\")\n",
    "answers = read_corpus(\"Answer_combined.dat\")\n",
    "\n",
    "assert len(questions)==len(answers), \"问题和答案列表的大小不一样，请检查读入数据是否有误!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二部分： 处理已有的字符串数据，并把它们转换成词袋向量。这部分内容涉及到一些简单的字符串预处理技术（比如过滤掉一些没用的字符、分词等），还有就是基于sklearn的把字符串转换向量的过程。本部分的内容需要字符串操作、分词、词袋模型相关的基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef filter_out_category(input_):#过滤词\\n    # TODO \\n    new_input = []\\n    for i in range(len(input_)):\\n        txt = input_[i]\\n        for ch in \"百度知道娱乐休闲音乐欧美流行教育科学习帮助的是哇吗啦呀啊哦嗯嘛了\":\\n            txt = txt.replace(ch,\"\")\\n        new_input.append(txt)\\n    return new_input\\n\\ndef filter_out_punctuation(input_):#标点符号\\n    # TODO\\n    new_input = []\\n    for i in range(len(questions)):\\n        txt = input_[i]\\n        for ch in \"~!@#$%^&*()_+·`，～（）。、；“！ ．？”△∠°＞＜‘’＂【…】《》1234567890QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm-={}[]\\\\|;\\':<>,./?\":\\n            txt = txt.replace(ch,\"\").replace(\\'\\n\\',\\'\\').replace(\\'\\t\\',\\'\\').replace(\\'\"\\',\\'\\')\\n        new_input.append(txt)\\n    return new_input\\n\\nimport jieba\\ndef word_segmentation(input_):#分词法\\n    # TODO\\n    new_input = []\\n    for i in range(len(input_)):\\n        txt = jieba.lcut(input_[i])\\n        new_input.append(\\' \\'.join(txt))\\n    return new_input\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def filter_out_category(input_):#过滤词\n",
    "    # TODO \n",
    "    new_input = []\n",
    "    for i in range(len(input_)):\n",
    "        txt = input_[i]\n",
    "        for ch in \"百度知道娱乐休闲音乐欧美流行教育科学习帮助的是哇吗啦呀啊哦嗯嘛了\":\n",
    "            txt = txt.replace(ch,\"\")\n",
    "        new_input.append(txt)\n",
    "    return new_input\n",
    "\n",
    "def filter_out_punctuation(input_):#标点符号\n",
    "    # TODO\n",
    "    new_input = []\n",
    "    for i in range(len(questions)):\n",
    "        txt = input_[i]\n",
    "        for ch in \"~!@#$%^&*()_+·`，～（）。、；“！ ．？”△∠°＞＜‘’＂【…】《》1234567890QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm-={}[]\\|;':<>,./?\":\n",
    "            txt = txt.replace(ch,\"\").replace('\\n','').replace('\\t','').replace('\"','')\n",
    "        new_input.append(txt)\n",
    "    return new_input\n",
    "\n",
    "import jieba\n",
    "def word_segmentation(input_):#分词法\n",
    "    # TODO\n",
    "    new_input = []\n",
    "    for i in range(len(input_)):\n",
    "        txt = jieba.lcut(input_[i])\n",
    "        new_input.append(' '.join(txt))\n",
    "    return new_input\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_category(input_):#过滤词\n",
    "    # TODO \n",
    "    for ch in \"百度知道娱乐休闲音乐欧美流行教育科学习帮助的是哇吗啦呀啊哦嗯嘛了\":\n",
    "        input_ = input_.replace(ch,\"\")\n",
    "        new_input = input_\n",
    "    return new_input\n",
    "\n",
    "def filter_out_punctuation(input_):#标点符号\n",
    "    # TODO\n",
    "    for ch in \"~!@#$%^&*()_+·`，～（）。、；“！ ．？”△∠°＞＜‘’＂【…】《》1234567890QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm-={}[]\\|;':<>,./?\":\n",
    "        input_ = input_.replace(ch,\"\").replace('\\n','').replace('\\t','').replace('\"','')\n",
    "        new_input = input_\n",
    "    return new_input\n",
    "\n",
    "import jieba\n",
    "def word_segmentation(input_):#分词\n",
    "    input_ = jieba.lcut(input_)\n",
    "    new_input = ' '.join(input_)\n",
    "    return new_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def convert2BOW(data):\n",
    "    # 把问题里的每一句话表示成词袋模型的向量。 比如把一个问题 - “贪心学院是做什么的？”，转换成一个向量（0,1,2,0,0...）类似这种。\n",
    "    # 函数的输入为<questions>变量，是一个list, list里的每一个entry是一个问题。所以转化的过程需要循环list里的每一个问题，然后把每一个问题\n",
    "    # 转换成对应的向量（词袋）。 \n",
    "    \n",
    "    new_data = []    # 为了理解的方便，创建一个新的list，用来存放字符串处理后的结果。 \n",
    "    for q in data:\n",
    "        # 对于每一个问题的String, 我们需要做一系列的预处理过程。\n",
    "        \n",
    "        # 第一步： 过滤掉问题前面的分类，比如每一行基本由“百度知道/娱乐休闲/音乐/欧美流行乐/” 这类字符开始，我们首先要去掉这部分\n",
    "        #   比如对于“百度知道/教育/科学/学习帮助/\t怎么写读书笔记啊？\t100-200字左右，最好有范文。。。。。。” 处理后的结果是\n",
    "        #   \"怎么写读书笔记啊？\t100-200字左右，最好有范文。。。。。。\"\n",
    "        q = filter_out_category(q)  \n",
    "        \n",
    "        # 第二步： 去掉句子里出现的所有的符号比如“?.,。”等等。 对于“贪心学院是做什么的？”，我们通过处理获得“贪心学院是做什么的”\n",
    "        q = filter_out_punctuation(q)\n",
    "        \n",
    "        # 第三步： 对句子分词，比如对于“贪心学院是做什么的”， 我们得到“贪心 学院 是 做 什么的”。 分词之后的词条由空格来分离\n",
    "        #   这部分需要用jieba库来实现，这部分的内容请参考 https://github.com/fxsjy/jieba\n",
    "        q = word_segmentation(q)\n",
    "        \n",
    "        # 把处理后的结果存到new_data里面\n",
    "        new_data.append(q)\n",
    "    \n",
    "    # TODO 利用skilearn工具，把过滤后的数据直接转换成词袋向量. 利用sklearn里面的CountVectorizer，即可以很容易获得词袋向量\n",
    "    # 参考：http://scikit-learn.org/stable/modules/feature_extraction.html (Section 4.2.3)\n",
    "    vectorizer = CountVectorizer(max_features=10000)  #  如果需要，请指定具体的参考。\n",
    "    # 通过vectorizer把字符串转换成词袋向量。 X是一个sparse matrix， 请思考为什么需要存储在sparse_matrix里面？ \n",
    "    X = vectorizer.fit_transform(new_data)\n",
    "    \n",
    "    # 返回已经build好的vectorizer object, 和已经转换过的词袋向量\n",
    "    return vectorizer, X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Leo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.595 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 把<questions>里面的每一个问题转换成词袋向量，把结果存在一个sparse matrix里面。\n",
    "vectorizer, X = convert2BOW(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三部分： 对于用户的新输入，返回答案。 这是最后一部分，也就是等我们创建完词袋向量之后，我们就可以输入一些新的问题，然后从库中找出最合适的答案。这部分的任务涉及到余弦相似度、简单搜索排序等方面基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def idx_for_largest_cosine_sim(input_, questions):\n",
    "    # TODO: 计算input和每个问题之间的余弦相似度，返回相似度最高的index. \n",
    "    best, best_idx = 0,0 #best：余弦相似度最大值；best_idx：最佳匹配标签\n",
    "    input_ = input_.toarray() #输入值转成array\n",
    "    length = np.linalg.norm(input_) #计算输入值的|模|\n",
    "    questions = questions.toarray() #把questions也转成array\n",
    "    for i in range(len(questions)):\n",
    "        a = questions[i]\n",
    "        b = np.dot(a,input_[0])/(np.linalg.norm(a)*length) #计算余弦相似度：a·b/(|a|*|b|)\n",
    "        if b>= best:\n",
    "            best , best_idx = b, i\n",
    "    return best_idx\n",
    "\n",
    "def answer(input_):\n",
    "    # 最后一步： 对于用户输入的新的问题，从库中返回最佳答案。 这个过程等同于“对于用户输入的问题，寻找库中最匹配的问题\n",
    "    # 然后返回对应那个问题的答案即可”\n",
    "    # 对于用户的输入字符串，我们同样首先需要做预处理\n",
    "    input_ = filter_out_punctuation(input_)\n",
    "    input_ = word_segmentation(input_)\n",
    "    \n",
    "    # TODO: 把输入的字符串转换成词袋向量, 可以使用 vectorizer.transform 函数。 这里的vectorizer是在已有的数据上已经训练过的\n",
    "    bow = vectorizer.transform([input_])\n",
    "    \n",
    "    # TODO: 有了对当前输入语句的词袋向量之后，我们需要跟库里的每一个问题做相似度计算，并返回相似度最高的问题的答案\n",
    "    best_idx = idx_for_largest_cosine_sim(bow, X)\n",
    "    \n",
    "    # 返回对应的问题的答案\n",
    "    return answers[best_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda\\ANACONDA-2019\\envs\\TF-CPU3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carlet\t描述系统的特征、动态不随空间坐标变化的模型,通常把系统看作一个整体,只研究输入(激发因素)与输出(响应)之间的拟合关系,而对系统内部的过程和机理不予考虑。把所有要素都看作属于空间一个点。模型可分为稳定的(与时间变量无关)和非稳定的(与时间变量有关)；线性的和非线性的等等。|`| 冰心的爱情\t 参数与非参数模型 用代数方程、微分方程、微分方程组以及传递函数等描述的模型都是参数模型。建立参数模型就在于确定已知模型结构中的各个参数。通过理论分析总是得出参数模型。非参数模型是直接或间接地从实际系统的实验分析中得到的响应，例如通过实验记录到的系统脉冲响应或阶跃响应就是非参数模型。运用各种系统辨识的方法，可由非参数模型得到参数模型。如果实验前可以决定系统的结构，则通过实验辨识可以直接得到参数模型。\n",
      "\n",
      "hunter94425\t不要用买的\\r\\n买了你看了一次就不想看了\\r\\n还不如看在现漫画\\r\\nhttp://www.9lala.com/Html/JJS/|`| 蓝色忧郁YIW\t 挺不好找的，去电驴下吧，1-21，速度挺快的。\\r\\nhttp://www.verycd.com/topics/195151/|`| 7852780\t http://comic.duowan.com/080819/jjs0008081914/index.html|`| k41886\t 淘淘网试一下吧！\n",
      "\n",
      "catrgb88\t电信只管线路和电信猫，其他不管。|`| zhangzichao01\t 肯定不会管的。因为电信不允许私自安装路由器的。如果确定是路由器的故障就需要自行找售后（如果在质保期内的话）或者更换了。|`| fallaimee\t 不管的，因为不是电信自家的问题，路由器有问题找厂商售后\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 可以做一些简单的测试，来看看效果，你也可以尝试打印最相近的问题\n",
    "print(answer(\"SVM是属于参数模型还是非参数模型？\"))\n",
    "print(answer(\"谁知道网上找兼职工作的网站\"))\n",
    "print(answer(\"路由器越用越卡是怎么回事？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四部分： 这部分包含一个思考题，想想通过什么样的优化或者改进使得系统能够实时返回答案？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 思考题： 当实现之后会发现，answer()函数运行会很慢，能简单说明一下这么慢的主要原因是什么？ \n",
    "#       如何优化answer（）函数使得更够实时提供回复呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你，通过本次的练习，我相信你已经掌握了一个问答系统的基本工作原理。当然，实际的问答系统会复杂很多，需要在每一个细节上做更多的事情，\n",
    "但即便这样，核心运行机制还是离不开本项目所涉及到的框架。这是你完成的第一个人工智能项目，其实就这么简单！ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
